{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai; openai.api_key = open('open_ai.key', 'r').read()\n",
    "import os\n",
    "os.environ['HTTP_PROXY'] = \"http://proxy.mei.co.jp:8080\"\n",
    "os.environ['HTTPS_PROXY'] = \"http://proxy.mei.co.jp:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_complete(messages, max_tokens=1024, temperature=0.5, debug=False):\n",
    "    stop = False\n",
    "    res = ''\n",
    "    if debug:\n",
    "        for m in messages:\n",
    "            print(m[\"role\"],'\\t',\"-\"*30)\n",
    "            print(m[\"content\"])\n",
    "            print()\n",
    "    total_tokens = 0\n",
    "    last_token = 0\n",
    "    while not stop:\n",
    "        success = False\n",
    "        while not success:\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo-0301\", messages=messages,\n",
    "                    temperature=temperature, max_tokens=max_tokens\n",
    "                )\n",
    "                success = True\n",
    "                res += response.choices[0].message.content\n",
    "                total_tokens += response.usage.total_tokens\n",
    "                last_token = response.usage.total_tokens\n",
    "                # if debug:\n",
    "                #     print(json.dumps(response.usage),response.choices[0].message.content.replace('\\n',' '))\n",
    "                if response.choices[0].finish_reason == 'stop':\n",
    "                    stop = True\n",
    "                else:\n",
    "                    messages.append({\"role\": 'user', \"content\": response.choices[0].message.content})\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                # time.sleep(5)\n",
    "    if debug:\n",
    "        print('assistant','\\t',\"-\"*30)\n",
    "        print(res, '\\n')\n",
    "        print('usage','\\t',\"-\"*30)\n",
    "        print('last:', last_token, 'total:', total_tokens)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text,debug=False):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': \"あなたの役割は論文を翻訳することです。\"},\n",
    "        {'role': 'user', 'content': text},\n",
    "    ]\n",
    "    res = chat_complete(messages,max_tokens=4000, temperature=0., debug=debug)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "    See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0301\")\n",
    "def tik(text):\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample text. 0 6\n",
      "It contains multiple sentences. 6 5\n",
      "Let's see if it works! 11 7\n",
      "['This is a sample text. It contains multiple sentences.', \"Let's see if it works!\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"This is a sample text. It contains multiple sentences. Let's see if it works!\"\n",
    "max_tokens = 15\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "chunks = []\n",
    "chunk = \"\"\n",
    "for sentence in sentences:\n",
    "    print(sentence, tik(chunk), tik(sentence))\n",
    "    if (tik(chunk)+tik(sentence)) <= max_tokens:\n",
    "    # if len(chunk.split()) + len(sentence.split()) <= max_tokens:\n",
    "        chunk += \" \" + sentence\n",
    "    else:\n",
    "        chunks.append(chunk.strip())\n",
    "        chunk = sentence\n",
    "if chunk:\n",
    "    chunks.append(chunk.strip())\n",
    "\n",
    "print(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
